---
title: "Maximum Likelihood"
date: "`r Sys.Date()`"
author: Claas Heuer
output:
  rmdformats::downcute:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
    use_bookdown: false
    # Rscript -e 'library(rmarkdown); render("3_Maximum_Likelihood.rmd", output_file = "./out/3_Maximum_Likelihood.html")'
---

```{r, include = FALSE}
library(pacman)
p_load(data.table, ggplot2, rrBLUP, lattice, brms, knitr, plotly, prettydoc, lubridate, DT, lsmeans, car, htmltools, pander, tidyverse)
interactive = FALSE
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c('top', 'right'))
```

```{r global_options, include=FALSE}

######################
### General Config ###
######################

knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
		      echo=FALSE, warning=FALSE, message=FALSE, include = FALSE, eval = TRUE)

fontSize = 15

```

  
# Maximum Likelihood

Maximum Likelihood is a parameterized optimization framwework that aims to find
the most likely parameters given a set of observations:

$$
L = f(y_1,y_2, y_3, ..., y_n | \theta) = f(y_1 | \theta) \times f(y_2 | \theta) \times f(y_3 | \theta) \times  ... \times f(y_3 | \theta)
$$

The overall likelhood is therefore the product of the likelihood of every single observation given the parameters.
This can become a very small product that generates numerical problems for the optimzation algorithm,
therefore it is often rephrased as an equivalent log-likelihood function:

$$
\log L = \sum_{i = 1}^{n}  \log (y_i | \theta)
$$

## Finding the mean of a normal distribution

$$
f(y) = \frac{1}{\sigma \sqrt{2\pi}} \mathrm{e}^{-0.5 ((x - \mu) / \sigma)^2}
$$

The normal distribution has two parameters ("moments"), the mean: $\mu$ and the standard deviation: $\sigma$.
The likelihood function of the normal distribution return the product of the individual likelihoods of all
observations, given the provided paramters ($\mu$ and $\sigma$).

A normal distribution (or any function) can be plotted in R with `curve`:

```{r nd, include = TRUE, echo = TRUE}
curve(
      dnorm(
	    x,
	    mean = 10,
	    sd = 2),
      from = 0,
      to = 20,
      main = "Normal distribution"
)
```

In R, we can find the maximum likelihood estimator of the mean the following way:

```{r mean, include = TRUE, echo = TRUE}
# generate random normal variable 
mean = 4.5
sd = 1
n = 1000

y = rnorm(n, mean, sd)

# the likelihood function = L(y;.) = y ~ N(mu, sigma) * rho ~ U(-1,1)
lik <- function(par, y, sd = 1) {

	# we construct the covariance matrix out of the correlation coefficient
	mean = par[1]

	# the likelihood of y
	L <- dnorm(x = y, mean = mean, sd = sd, log=TRUE)

	# the joint likelihood
	return(-sum(L))

}


# the parameters we wish to estimate
p <- c(0)
names(p) <- c("mu")

# Get the Maximum Likelihood estimates for our unknowns
ML <- optim(p, lik, y=y)

# compare ML results to empirical mean
print("Maximum Likelihood:")
ML$par

print("Empirical")
mean(y)
```

## Excercise

Finding the maximum likelihood estimates for the mean **and** the standard deviation

## Estimating correlation of two multivariate normal random variables


```{r cor, include = TRUE, echo = TRUE}
library(mvtnorm)

# likelihood on correlation coefficient.
# this is essentially a uniform prior on
# the range -1 to 1 and 0 density outside
# that range
corlik <- function(x) {

	res <- 0
	if(x >= -1 & x <= 1) res = 1

	return(log(res))

}

# the likelihood for the variance components.
# uniform above 0 and zero if negative
varlik <- function(x) {

	res <- 0
	if(x > 0) res = 1

	return(log(res))

}

# the likelihood function = L(y;.) = y ~ MVN(mu, sigma) * rho ~ U(-1,1)
lik <- function(par, y) {

	# we construct the covariance matrix out of the correlation coefficient
	# and the variance components
	covAB <- par[5] * (sqrt(par[3]) * sqrt(par[4]))
	sigma <- array(c(par[3], covAB, covAB, par[4]), dim=c(2,2))

	# the likelihood of y
	L1 <- dmvnorm(x = Y, mean = c(par[1],par[2]), sigma = sigma, log=TRUE)

	# the prior = likelihood of rho (correlation coefficient)
	L2 <- corlik(par[5])

	# and the priors on the variance components
	L3 <- varlik(par[3])
	L4 <- varlik(par[4])

	# the joint likelihood
	return(-sum(L1 + L2 + L3 + L4))

}


# those are the optimizers available
methods = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN","Brent")

# the parameters we wish to estimate
p <- c(0,0,1,1,0.2)
names(p) <- c("mu1","mu2","tauA","tauB","rho")

# create random data
n = 10
tauA <- 3
tauB <- 10
rho = 0.7
covAB <- rho * (sqrt(tauA) * sqrt(tauB))
muA <- 5
muB <- 20

# the variance-covariance matrix
sigma <- array(c(tauA,covAB,covAB,tauB), dim=c(2,2))

# the random data
Y <- rmvnorm(n, c(muA,muB), sigma)

# Get the Maximum Likelihood estimates for our unknowns
ML <- optim(p, lik, y=y, method = methods[2])

res <- as.list(ML$par)
res$covAB <- res$rho * (sqrt(res$tauA) * sqrt(res$tauB))

# compare ML estimates with empiricals
out <- rbind(unlist(res), c(colMeans(Y), var(Y[,1]), var(Y[,2]), cor(Y)[1,2], cov(Y)[1,2]))
colnames(out) <- names(res)
rownames(out) <- c("ML","empirical")

# insepct results
out
```

# Animal Model - Linear Mixed Model solved by Maximum Likelihood

The linear mixed model is:

$$
\mathbf{y} = \mathbf{Xb} + \mathbf{a} + \mathbf{e},
$$

with

$$
\mathbf{y} \sim MVN(\mathbf{Xb}, \mathbf{A}\sigma_{a}^{2} + \mathbf{I} \sigma_{e}^{2})
$$



```{r am, include = TRUE, echo = TRUE}
# this is needed for the PDF of a multivariate normal distribution
library(mvtnorm)

# we use the BGLR data
library(BGLR)
data(wheat)

# the numerator relationship matrix
A <- wheat.A

# the cholesky of that (only used here to make the
# funcion operate more generally
L <- t(chol(wheat.A))

# the phenotype
y <- wheat.Y[,1]

# design matrices for the model: y = Xb + Zu + e, with e ~ N(0,var_e) and
# u ~ MVN(0,ZZ'var_a).
# above formulation was for the conditional model p(y|u) and p(u|.),
# but here we use: y = Xb + e, with e ~ MVN(0, ZZ' var_a + I var_e)
X <- matrix(1,length(y),1)
Z <- L

# the likelihood function = L(y;.) = prod(PDF(y, E(y) = Xb, var(y) = ZZ' var_a + I var_e))
lik <- function(par, y, X, Z) {

# it has to be this awkward because 'optim' only wants one vector of parameters
 b <- matrix(c(par[3:(2+ncol(X))]), ncol=1)

# this is ZZ' var_a
 A <- tcrossprod(Z) * par[2]

# this is I var_e
 R <- diag(length(y)) * par[1]

 V = A + R

# E(y)
 mu <- X %*% b

# optim minimizes - therefor we need the negative product of
# likelihoods for every i.i.d. random variable in y given the model
# and parameters.
# Instead of taking the products over the likelihoods we take
# the sum over log(L) for numerical stability
 -(sum(dmvnorm(x = y,mean = mu ,sigma = V, log=TRUE)))

}


# This is only necessary because 'optim' wants one parameter vector
# from here: http://stackoverflow.com/a/7573077/2748031
getp <- function(X, Z = NULL, b = 0.1, u= 0.1, var_e=1 ,var_a = 1) {

  if(is.null(Z)) { 

    p <- c(var_e, var_a, rep(b, ncol(X)))
    names(p) <- c("var_e", "var_a", paste("b", 0:(ncol(X)-1), sep=""))

  } else {

        p <- c(var_e, var_a, rep(b, ncol(X)), rep(u, ncol(Z)))
        names(p) <- c("var_e", "var_a", paste("b", 0:(ncol(X)-1), sep=""),paste("u", 1:(ncol(Z)), sep=""))

      }

  p

}


# those are the optimizers available 
methods = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN","Brent")

# Get the Maximum Likelihood estimates for our unknowns
ML <- optim(getp(X), lik, X=X, Z=Z, y=y, method = methods[1])

# compare (NOTE: mixed.solve does REML not ML)
library(rrBLUP)
mod <- mixed.solve(y=y, X=X, Z=Z)
```



